{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Collection\n",
    "- Collect links for all the archives\n",
    "- For each archive, collect all the articles (main body and categories), and store them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://mlg.ucd.ie/modules/COMP41680/archive/'\n",
    "index_url = 'http://mlg.ucd.ie/modules/COMP41680/archive/index.html' # I use this\n",
    "\n",
    "# main lists\n",
    "month_links = []\n",
    "articles = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this step, I get all links from each month from the main page\n",
    "\n",
    "req = requests.get(index_url)\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "list_items = soup.find_all('li')\n",
    "\n",
    "# month_links = [base_url + li.find('a').get('href') for li in list_items]\n",
    "\n",
    "for li in list_items:\n",
    "    link_item = li.find('a').get('href')\n",
    "    month_links.append(base_url + link_item)\n",
    "\n",
    "month_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this step, from each link from each month_link, I get the category, title and link of each article\n",
    "import urllib.request\n",
    "id = 1\n",
    "article_category = dict()\n",
    "for month_link in month_links:\n",
    "    req = requests.get(month_link)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    \n",
    "    tbody = soup.find('tbody')\n",
    "    article_rows = tbody.find_all('tr')\n",
    "    \n",
    "    for article_row in article_rows:\n",
    "        category = article_row.find(class_='category').text\n",
    "        reference_elem = article_row.find('a')\n",
    "        \n",
    "        #here I filter all the articles with no content\n",
    "        if reference_elem is None:\n",
    "            continue\n",
    "        else:\n",
    "            # Store the category for current article.\n",
    "            article_category[str(id)] = category\n",
    "                \n",
    "            title = reference_elem.text\n",
    "            link = base_url + reference_elem.get('href')\n",
    "            article_response = urllib.request.urlopen(link)\n",
    "            article_html = article_response.read().decode()\n",
    "            soup = BeautifulSoup(article_html, 'html.parser') \n",
    "            \n",
    "            # Get the content/body text from articles\n",
    "            for element in soup.find_all(\"body\"):\n",
    "                article_text = \"\"\n",
    "                article_text += element.find(\"h2\").text\n",
    "                article_text += \" \"\n",
    "                \n",
    "                for p in element.find_all(\"p\"):\n",
    "                    if \"Return to article search results\" in p.text:\n",
    "                        continue\n",
    "                    else:\n",
    "                        article_text += p.text\n",
    "                \n",
    "                # Store the content/body text from articles in a separate file.\n",
    "                content_file = open(str(id) + \".txt\", \"w\", encoding=\"utf-8\")\n",
    "                content_file.write(article_text)\n",
    "                content_file.close()\n",
    "                id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newfile = open(\"categories4.csv\", \"w\", encoding=\"utf-8\")\n",
    "content = \"id,category\\n\"\n",
    "for id in article_category:\n",
    "    content += id\n",
    "    content += \",\"\n",
    "    content += article_category[id]\n",
    "    content += \"\\n\"\n",
    "newfile.write(content)\n",
    "newfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load the set of raw documents in notebook with class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "f = pd.read_csv(\"categories4.csv\")\n",
    "f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the file is correct with all the articles\n",
    "content_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_bodies = []\n",
    "import numpy as np\n",
    "id_list = f.get(\"id\")\n",
    "for id in id_list:\n",
    "    article_file = open(str((id))+\".txt\", \"r\", encoding=\"utf-8\")\n",
    "    article_text = article_file.read()\n",
    "    article_file.close()\n",
    "    article_bodies.append(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_bodies[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a document-term matrix, using appropriate text pre-processing and term weighting steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Read %d raw text documents\" % len(article_bodies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"categories4.csv\")\n",
    "df['body'] = article_bodies\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "articles = df[\"body\"]\n",
    "    \n",
    "def lemmatizer_token(text):\n",
    "\n",
    "    tokens=nltk.tokenize.word_tokenize(text)\n",
    "    lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "    lemma_tokens= []\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        lemma_tokens.append(lemmatizer.lemmatize(token) )\n",
    "    return lemma_tokens\n",
    "\n",
    "vectorizer=TfidfVectorizer(stop_words=\"english\", tokenizer = lemmatizer_token, min_df = 5)\n",
    "tfidf =vectorizer.fit_transform(df[\"body\"])\n",
    "document =  vectorizer.get_feature_names()\n",
    "\n",
    "tf_idf =  tfidf.toarray()\n",
    "tf_df = pd.DataFrame(data=tf_idf, index = df['id'], columns= document)\n",
    "tf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build two multi-class classification models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "X=tf_idf\n",
    "y=df['category']\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X,y)\n",
    "score1 = cross_val_score(model,X,y,cv=5)\n",
    "print(score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model2 = GaussianNB()\n",
    "model2.fit(X,y)\n",
    "score2 = cross_val_score(model2,X,y,cv=5)\n",
    "print(score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn.metrics import confusion_matrix\n",
    "predicted = model.predict(X)\n",
    "cm = confusion_matrix(y, predicted)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB\n",
    "predicted = model2.predict(X)\n",
    "cm2 = confusion_matrix(y, predicted)\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vissualitation the evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.hist(score1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist((score2), color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure()\n",
    "plt.plot(score2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conclusion:\n",
    " With all the data stored and having created the document-term matrix, the next step has been create two multi-class classification models. It has been used tthe k-Nearest Neighbour Classifier and the Naive Bayes classifier . From the results obtained by both, the KNN classifier has more accurate scores than the NB classifier, therefore the first one has more precision., and in order to check that in the evaluation stage to measure the Accuracy I used the confusion matrix as a tool, which in the visual representation it can be seen larger accomulation of higher scores with respect to the first classifier than with the second one. therefore I can affirm that the first classifier(KNN) has worked better than the second one(NB)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
